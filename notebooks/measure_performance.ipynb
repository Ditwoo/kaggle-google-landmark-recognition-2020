{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from metrics import gap\n",
    "from models import EncoderWithHead\n",
    "from models.efficientnets import EfficientNetEncoder\n",
    "from models.heads import CosFace\n",
    "from datasets import FolderDataset\n",
    "from batteries.progress import tqdm\n",
    "from batteries import t2d, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Num records in valid dataset - 72322, batches - 283\n"
     ]
    }
   ],
   "source": [
    "train_valid = ps.read_pickle(\"../input/train_valid.pkl\")\n",
    "IMAGES_DIR = Path(\"..\") / \"input\" / \"train\"\n",
    "\n",
    "landmark_map = {\n",
    "    landmark: idx\n",
    "    for idx, landmark in enumerate(sorted(set(train_valid[\"landmark_id\"].values)))\n",
    "}\n",
    "\n",
    "valid = train_valid[train_valid[\"is_valid\"] == True]\n",
    "valid_set = FolderDataset(\n",
    "    valid[\"id\"].values,\n",
    "    valid[\"landmark_id\"].values,\n",
    "    landmark_map,\n",
    "    data_dir=IMAGES_DIR,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_set, batch_size=256, num_workers=16\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\" * Num records in valid dataset - {len(valid_set)}, batches - {len(valid_loader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "<= Loaded model from '../logs/full_set2/stage_0/best.pth'\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 512\n",
    "NUM_CLASSESS = len(landmark_map)\n",
    "\n",
    "\n",
    "model = EncoderWithHead(\n",
    "    EfficientNetEncoder(\"efficientnet-b0\", EMBEDDING_SIZE, bias=False),\n",
    "    CosFace(EMBEDDING_SIZE, NUM_CLASSESS, None),\n",
    ")\n",
    "load_checkpoint(\"../logs/full_set2/stage_0/best.pth\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head.s = np.sqrt(2) * np.log(NUM_CLASSESS - 1)\n",
    "model.head.m = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|████████████████████| 283/283 [01:52<00:00,  2.52it/s, loss 3.7170, gap 0.3590, acc 0.5385]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"gap\": [],\n",
    "}\n",
    "\n",
    "with torch.no_grad(), tqdm(total=len(valid_loader), desc=\"valid\") as progress:\n",
    "    for _idx, batch in enumerate(valid_loader):\n",
    "        inputs, targets = t2d(batch, device)\n",
    "\n",
    "        outputs = model(inputs, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        _loss = loss.detach().item()\n",
    "        metrics[\"loss\"].append(_loss)\n",
    "        \n",
    "        classes = torch.argmax(outputs, 1)\n",
    "        _acc = (classes == targets).float().mean().detach().item()\n",
    "        metrics[\"accuracy\"].append(_acc)\n",
    "\n",
    "        confidences, predictions = torch.max(outputs, dim=1)\n",
    "        _gap = gap(predictions, confidences, targets)\n",
    "        metrics[\"gap\"].append(_gap)\n",
    "\n",
    "        progress.set_postfix_str(f\"loss {_loss:.4f}, gap {_gap:.4f}, acc {_acc:.4f}\")\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.763266608908825, 'accuracy': 0.5353611716112062, 'gap': 0.38452192799212787}\n"
     ]
    }
   ],
   "source": [
    "metrics[\"loss\"] = np.mean(metrics[\"loss\"])\n",
    "metrics[\"accuracy\"] = np.mean(metrics[\"accuracy\"])\n",
    "metrics[\"gap\"] = np.mean(metrics[\"gap\"])\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Num records in valid dataset - 1508148, batches - 5892\n"
     ]
    }
   ],
   "source": [
    "train = train_valid[train_valid[\"is_valid\"] == False]\n",
    "train_set = FolderDataset(\n",
    "    train[\"id\"].values,\n",
    "    train[\"landmark_id\"].values,\n",
    "    landmark_map,\n",
    "    data_dir=IMAGES_DIR,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set, batch_size=256, num_workers=16\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\" * Num records in valid dataset - {len(train_set)}, batches - {len(train_loader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████| 5892/5892 [37:52<00:00,  2.59it/s, loss 20.9686, gap 0.0000, acc 0.0000]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"gap\": [],\n",
    "}\n",
    "\n",
    "with torch.no_grad(), tqdm(total=len(train_loader), desc=\"train\") as progress:\n",
    "    for _idx, batch in enumerate(train_loader):\n",
    "        inputs, targets = t2d(batch, device)\n",
    "\n",
    "        outputs = model(inputs, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        _loss = loss.detach().item()\n",
    "        metrics[\"loss\"].append(_loss)\n",
    "        \n",
    "        classes = torch.argmax(outputs, 1)\n",
    "        _acc = (classes == targets).float().mean().detach().item()\n",
    "        metrics[\"accuracy\"].append(_acc)\n",
    "\n",
    "        confidences, predictions = torch.max(outputs, dim=1)\n",
    "        _gap = gap(predictions, confidences, targets)\n",
    "        metrics[\"gap\"].append(_gap)\n",
    "\n",
    "        progress.set_postfix_str(f\"loss {_loss:.4f}, gap {_gap:.4f}, acc {_acc:.4f}\")\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 32.26586946822799, 'accuracy': 2.8507934487440598e-05, 'gap': 5.071256240721227e-07}\n"
     ]
    }
   ],
   "source": [
    "metrics[\"loss\"] = np.mean(metrics[\"loss\"])\n",
    "metrics[\"accuracy\"] = np.mean(metrics[\"accuracy\"])\n",
    "metrics[\"gap\"] = np.mean(metrics[\"gap\"])\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37464bittorchcondab46d9803850a4193b40c5aded830a323"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
